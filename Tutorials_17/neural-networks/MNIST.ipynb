{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Last tutorial we created the entire graph and then launched the session. \n",
    "# There is also an interactive session, which I think is useful while you are putting\n",
    "# together a new model.  You can run your graph in parts as you add to it, which\n",
    "# makes it useful for debugging and experimenting.\n",
    "\n",
    "# In this tutorial, we'll use the interactive graph to 'simulate' how we would\n",
    "# experiment with and assemble a model a piece at a time.  Once we have finished\n",
    "# putting the model together we re-assemble it \"for real\" and run it in a single sesion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we will load the data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start the interactive session\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remember that placeholders are promises to provide data.\n",
    "# First we'll make placeholders for the input data and the output.\n",
    "# The input data are 784 pixel images, and the output will be a 10\n",
    "# dimensional vector, each corresponding to a number 0-9.\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])  # Pixel intensities\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])  # Training labels\n",
    "\n",
    "# None lets us put any size in that dimension of x and y.  So each image\n",
    "# will have 784 pixels, and occupies a single row in the tensor.  Since we\n",
    "# did not specify the number of rows, we are free to provide as many as\n",
    "# we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some helper functions for building the neural network.  These just create variables\n",
    "# of a certain shape and randomly initialize them.\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we will initialize the weights.  This will be variables, not\n",
    "# placeholders, since they will be updated by tensorflow.  Parameters to be\n",
    "# learned are typically variables in tensorflow.\n",
    "\n",
    "hidden_nodes = 25\n",
    "\n",
    "W = weight_variable([784,hidden_nodes])  # A 784 x 10 matrix\n",
    "b = bias_variable([hidden_nodes])\n",
    "\n",
    "h = tf.nn.relu(tf.matmul(x, W) + b)\n",
    "\n",
    "W_output = weight_variable([hidden_nodes, 10])\n",
    "b_output = bias_variable([10])\n",
    "\n",
    "# We are implementing a softmax model.  Think of it like logistic regression\n",
    "# on more dimensions than 2.  \n",
    "#\n",
    "#                             P(y = j | x) = exp(x*w_j) / sum(exp(x*W + b))\n",
    "#\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the unscaled logits.  This is what Tensorflow wants, so this is\n",
    "# what tensorflow gets.  I guess that unscaled means that we don't exponentiate\n",
    "# the output either.  The tensorflow function we are about to call takes care \n",
    "# of all this.  I have to say the tutorial/documentation are opaque here.\n",
    "\n",
    "y = tf.matmul(h,W_output) + b_output\n",
    "\n",
    "# cross_entropy is not a good name for this function.  We should call it loss, but\n",
    "# I am going to leave it like this to match the tutorial.\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))   \n",
    "\n",
    "# tf.nn.softmax_etc calculates the cross entropy of one sample.  We take the mean \n",
    "# because that is convention.  \n",
    "# See https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define a train_step for the model.  Just one step.\n",
    "# Minimize loss function using gradient descent.\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now run the train step 1000 times and see where we are at.\n",
    "\n",
    "for _ in range(1000):\n",
    "  batch = mnist.train.next_batch(100)  # Grab 100 training instsances.\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1]}) # Give the training batch to x and y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that the model has been run, we evaluate the models' performance.\n",
    "#\n",
    "# The correct way to do this is to add ops to the graph and then execute all at once.\n",
    "# One complaint I have about this is that it makes it difficult to build your code in parts...\n",
    "# Even with the interactive session, it is unwieldy to see how tensors move through the\n",
    "# graph.\n",
    "\n",
    "# First, we get the predicted labels for each label for each model and compare it to the \n",
    "# correct label.\n",
    "\n",
    "correct_label = tf.equal(tf.arg_max(y, 1), tf.arg_max(y_,1)) # This returns integers I guess.\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_label, tf.float32))  # Reduce mean needs floats, so cast to float32\n",
    "\n",
    "print(accuracy.eval(feed_dict={x:mnist.test.images, y_:mnist.test.labels}))\n",
    "print(sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
